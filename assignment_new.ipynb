{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from brats.load_data import load_data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of samples: 167\nThe number of columns: 725\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of samples: 167\nThe number of columns: 705\n"
     ]
    }
   ],
   "source": [
    "remove_features = data.T[data.isna().sum(axis=0) < 130]\n",
    "removed_features = remove_features.T\n",
    "print(f'The number of samples: {len(removed_features)}')\n",
    "print(f'The number of columns: {len(removed_features.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute Nans\n",
    "\n",
    "label = removed_features['label'].to_numpy() #extract labels\n",
    "features = removed_features.drop('label', axis=1) #dropped laatste kolom (labels)\n",
    "replace_div = features.replace(r'#DIV/0!', 'nan', regex=True) #DIV's worden vervangen door nan \n",
    "replace_inf = replace_div.replace(np.inf, np.nan, regex=True) #inf wordt vervangen door nan\n",
    "\n",
    "feature_names = list(replace_inf.columns) #extract feature names\n",
    "imputer = KNNImputer(n_neighbors=10) #definieert functie K-nearest neighbor imputer\n",
    "clean_data = imputer.fit_transform(replace_inf) #alle nan's worden vervangen op basis van k-NN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels to true and false\n",
    "labels = label=='GBM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'classifier': RandomForestClassifier(max_depth=8, max_leaf_nodes=10, min_samples_leaf=5), 'classifier__max_depth': 8, 'classifier__max_leaf_nodes': 10, 'classifier__min_samples_leaf': 5, 'classifier__n_estimators': 100}\n",
      "{'classifier': RandomForestClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=2), 'classifier__max_depth': 5, 'classifier__max_leaf_nodes': 10, 'classifier__min_samples_leaf': 2, 'classifier__n_estimators': 100}\n",
      "{'classifier': RandomForestClassifier(max_depth=8, max_leaf_nodes=10, min_samples_leaf=2), 'classifier__max_depth': 8, 'classifier__max_leaf_nodes': 10, 'classifier__min_samples_leaf': 2, 'classifier__n_estimators': 100}\n",
      "{'classifier': RandomForestClassifier(max_depth=25, max_leaf_nodes=5, min_samples_leaf=10), 'classifier__max_depth': 25, 'classifier__max_leaf_nodes': 5, 'classifier__min_samples_leaf': 10, 'classifier__n_estimators': 100}\n",
      "{'classifier': RandomForestClassifier(max_depth=15, max_leaf_nodes=10, min_samples_leaf=2), 'classifier__max_depth': 15, 'classifier__max_leaf_nodes': 10, 'classifier__min_samples_leaf': 2, 'classifier__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "rskf = StratifiedKFold(n_splits=5) #ik heb random state verwijderd, daardoor worden steeds dezelfde r splits gemaakt\n",
    "auc= []\n",
    "accuracy= []\n",
    "F1= []\n",
    "precision = []\n",
    "recall= []\n",
    "for train_index, test_index in rskf.split(clean_data, labels):\n",
    "    data_train, data_test = clean_data[train_index], clean_data[test_index]\n",
    "    label_train, label_test = labels[train_index], labels[test_index]\n",
    "    # Create a pipeline\n",
    "    pipe = Pipeline([('scalar',StandardScaler()),('pca',PCA(n_components=0.95)),('classifier',RandomForestClassifier())])\n",
    "    # Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "    grid_param = [{\"classifier\": [RandomForestClassifier()],\n",
    "                   \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                   \"classifier__max_depth\":[5,8,15,25,30,None],\n",
    "                   \"classifier__min_samples_leaf\":[2,5,10,15,100],\n",
    "                   \"classifier__max_leaf_nodes\": [2, 5,10]}]\n",
    "    # create a gridsearch of the pipeline, the fit the best model\n",
    "    skf = StratifiedKFold(n_splits=7)\n",
    "    gridsearch = GridSearchCV(pipe, grid_param, cv=skf, verbose=0,n_jobs=-1) # Fit grid search\n",
    "    best_model = gridsearch.fit(data_train,label_train)\n",
    "\n",
    "    best_parameters = gridsearch.best_params_\n",
    "    print(best_parameters)\n",
    "\n",
    "    # # get roc/auc info\n",
    "    # y_score = best_model.predict_proba(data_test)[:,1]\n",
    "    # # fpr = dict()\n",
    "    # # tpr = dict()\n",
    "    # # fpr, tpr, _ = roc_curve(label_test, y_score)\n",
    "    # # roc_auc = dict()\n",
    "    # # roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # y_pred=best_model.predict(data_test)\n",
    "    # auc.append(metrics.roc_auc_score(label_test, y_score))\n",
    "    # accuracy.append(metrics.accuracy_score(label_test, y_pred))\n",
    "    # F1.append(metrics.f1_score(label_test,y_pred))\n",
    "    # precision.append(metrics.precision_score(label_test,y_pred))\n",
    "    # recall.append(metrics.recall_score(label_test, y_pred))\n",
    "\n",
    "\n",
    "    # make the plot\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # plt.xlim([-0.05, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.grid(True)\n",
    "    # plt.plot(fpr, tpr, label='AUC = {0}'.format(roc_auc))        \n",
    "    # plt.legend(loc=\"lower right\", shadow=True, fancybox =True) \n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8717948717948718, 0.8461538461538461, 0.9884615384615385, 0.9, 0.8653846153846153]\n[0.7058823529411765, 0.7647058823529411, 0.9090909090909091, 0.7878787878787878, 0.8181818181818182]\n[0.7999999999999999, 0.8333333333333334, 0.9302325581395349, 0.8444444444444444, 0.8636363636363635]\n[0.6896551724137931, 0.7407407407407407, 0.8695652173913043, 0.76, 0.7916666666666666]\n[0.9523809523809523, 0.9523809523809523, 1.0, 0.95, 0.95]\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(auc)\n",
    "print(accuracy)\n",
    "print(F1)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python385jvsc74a57bd085cbd929057288290dc69370c08c1388615b98615b7537bc4147a36af42d7eef",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}