{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from brats.load_data import load_data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of samples: 167\nThe number of columns: 725\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of samples: 167\nThe number of columns: 705\n"
     ]
    }
   ],
   "source": [
    "remove_features = data.T[data.isna().sum(axis=0) < 130]\n",
    "removed_features = remove_features.T\n",
    "print(f'The number of samples: {len(removed_features)}')\n",
    "print(f'The number of columns: {len(removed_features.columns)}')\n",
    "\n",
    "#remove_patients= removed_features[removed_features.isna().sum(axis=1) < 400]\n",
    "#print(f'The number of samples: {len(remove_patients)}')\n",
    "#print(f'The number of columns: {len(remove_patients.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaNs\n",
    "# clean_data = KNNImputer(n_neighbors=10)\n",
    "# clean_data.fit_transform(features)\n",
    "\n",
    "# without_labels = remove_patients.drop('label', axis=1)\n",
    "# without_labels.replace(r'#DIV/0!', 0)\n",
    "# clean_data = without_labels.replace(np.NaN, without_labels.mean(axis = 0))\n",
    "# clean_data.isna().sum().sum()\n",
    "\n",
    "#print(remove_patients.isna().sum().sum())\n",
    "#new = remove_patients.replace(r'#DIV/0!', 0, regex=True)\n",
    "#print(new.isna().sum().sum())\n",
    "#clean_data = new.replace(np.NaN, remove_patients.mean(axis = 0))\n",
    "#print(clean_data.isna().sum().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels and features\n",
    "#label = clean_data['label'].to_numpy()\n",
    "#features = clean_data.drop('label', axis=1).to_numpy()\n",
    "\n",
    "#features = removed_features.drop('label', axis=1)\n",
    "#replace_div = features.replace(r'#DIV/0!', 'nan', regex=True)\n",
    "#new = replace_div.replace(np.inf, np.nan, regex=True)\n",
    "\n",
    "label = removed_features['label'].to_numpy() #extract labels\n",
    "features = removed_features.drop('label', axis=1) #dropped laatste kolom (labels)\n",
    "replace_div = features.replace(r'#DIV/0!', 'nan', regex=True) #DIV's worden vervangen door nan \n",
    "replace_inf = replace_div.replace(np.inf, np.nan, regex=True) #inf wordt vervangen door nan\n",
    "\n",
    "feature_names = list(replace_inf.columns) #extract feature names\n",
    "imputer = KNNImputer(n_neighbors=10) #definieert functie K-nearest neighbor imputer\n",
    "clean_data = imputer.fit_transform(replace_inf) #alle nan's worden vervangen op basis van k-NN imputer\n",
    "#clean_data = pd.DataFrame(data=clean_data) #transform numpy array to dataframe\n",
    "#clean_data.columns = feature_names #add feature names\n",
    "\n",
    "\n",
    "#clean_data = replace_inf.replace(np.nan, removed_features.mean(axis = 0)) #alle nan's worden vervangen door mean van kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels to true and false\n",
    "labels = label=='GBM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_pca(data_train, data_test, label_train, label_test, j=20):\n",
    "    \"\"\"\n",
    "    This function calculates the PCA for j principal components\n",
    "    \"\"\"\n",
    "\n",
    "    #Standard Scaling\n",
    "    scaler = StandardScaler().fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "\n",
    "    #Train on trainingsdata and apply PCA\n",
    "    pca = PCA(n_components=j).fit(data_train)\n",
    "    train_array = pca.transform(data_train)\n",
    "    test_array = pca.transform(data_test)\n",
    "\n",
    "    #Transform to numpy and from array to float\n",
    "    label_train_array = label_train.squeeze().astype(np.int)\n",
    "    label_test_array = label_test.squeeze().astype(np.int)\n",
    "\n",
    "    return(train_array, test_array, label_train_array, label_test_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: [  6   7  10  11  12  16  20  21  24  26  27  29  34  35  37  39  41  43\n  45  48  50  51  53  54  55  56  58  61  62  65  67  68  70  73  76  79\n  80  84  85  87  89  90  91  92  93  95  97  98  99 100 101 102 106 111\n 112 113 115 117 118 120 121 123 128 130 131 138 140 142 145 148 151 152\n 153 154 155 156 157 158 159 160 161 162 165] TEST: [  0   1   2   3   4   5   8   9  13  14  15  17  18  19  22  23  25  28\n  30  31  32  33  36  38  40  42  44  46  47  49  52  57  59  60  63  64\n  66  69  71  72  74  75  77  78  81  82  83  86  88  94  96 103 104 105\n 107 108 109 110 114 116 119 122 124 125 126 127 129 132 133 134 135 136\n 137 139 141 143 144 146 147 149 150 163 164 166]\nTRAIN: [  0   1   2   3   4   5   8   9  13  14  15  17  18  19  22  23  25  28\n  30  31  32  33  36  38  40  42  44  46  47  49  52  57  59  60  63  64\n  66  69  71  72  74  75  77  78  81  82  83  86  88  94  96 103 104 105\n 107 108 109 110 114 116 119 122 124 125 126 127 129 132 133 134 135 136\n 137 139 141 143 144 146 147 149 150 163 164 166] TEST: [  6   7  10  11  12  16  20  21  24  26  27  29  34  35  37  39  41  43\n  45  48  50  51  53  54  55  56  58  61  62  65  67  68  70  73  76  79\n  80  84  85  87  89  90  91  92  93  95  97  98  99 100 101 102 106 111\n 112 113 115 117 118 120 121 123 128 130 131 138 140 142 145 148 151 152\n 153 154 155 156 157 158 159 160 161 162 165]\nTRAIN: [  1   2   3   4   5   8  10  11  17  18  19  22  25  27  30  31  33  36\n  38  39  40  48  49  50  54  56  57  58  60  61  63  64  65  66  69  70\n  72  73  78  79  80  81  82  84  88  89  90  92  97  98 100 105 106 107\n 108 114 115 116 117 119 120 121 122 125 126 130 131 135 137 139 140 144\n 145 146 147 148 152 156 158 160 161 163 165] TEST: [  0   6   7   9  12  13  14  15  16  20  21  23  24  26  28  29  32  34\n  35  37  41  42  43  44  45  46  47  51  52  53  55  59  62  67  68  71\n  74  75  76  77  83  85  86  87  91  93  94  95  96  99 101 102 103 104\n 109 110 111 112 113 118 123 124 127 128 129 132 133 134 136 138 141 142\n 143 149 150 151 153 154 155 157 159 162 164 166]\nTRAIN: [  0   6   7   9  12  13  14  15  16  20  21  23  24  26  28  29  32  34\n  35  37  41  42  43  44  45  46  47  51  52  53  55  59  62  67  68  71\n  74  75  76  77  83  85  86  87  91  93  94  95  96  99 101 102 103 104\n 109 110 111 112 113 118 123 124 127 128 129 132 133 134 136 138 141 142\n 143 149 150 151 153 154 155 157 159 162 164 166] TEST: [  1   2   3   4   5   8  10  11  17  18  19  22  25  27  30  31  33  36\n  38  39  40  48  49  50  54  56  57  58  60  61  63  64  65  66  69  70\n  72  73  78  79  80  81  82  84  88  89  90  92  97  98 100 105 106 107\n 108 114 115 116 117 119 120 121 122 125 126 130 131 135 137 139 140 144\n 145 146 147 148 152 156 158 160 161 163 165]\n"
     ]
    }
   ],
   "source": [
    "# Split data in training en test\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=36851234)\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2) #ik heb random state verwijderd, daardoor worden steeds dezelfde splits gemaakt\n",
    "# waarvoor was dit bedoeld? \n",
    "\n",
    "for train_index, test_index in rskf.split(features, label):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    data_train, data_test = clean_data[train_index], clean_data[test_index]\n",
    "    label_train, label_test = labels[train_index], labels[test_index]\n",
    "    train_array, test_array, label_train_array, label_test_array = our_pca(data_train, data_test, label_train, label_test)\n",
    "\n",
    "# PCA op training en test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}